{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalamos pyspark y py4j\n",
        "!pip install pyspark py4j\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "Spark = SparkSession.builder.appName(\"miriam\").getOrCreate()\n",
        "SparkContext = Spark.sparkContext\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65roiDw4F4Wj",
        "outputId": "29b030fb-73af-4372-9b84-a2bf077a123c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=411d0a642f8a66920dfdd96175fd3b252d5d9abba10be0743f5c262cb6517694\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: Cree un RDD llamado lenguajes que contenga los siguientes lenguajes de programación: Python, R, C, Scala, Rugby y SQL."
      ],
      "metadata": {
        "id": "IVcaN4Y8H9NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un RDD llamado lenguajes que contenga los siguientes lenguajes de programación: Python, R, C, Scala, Rugby y SQL.\n",
        "lenguajes = sc.parallelize(['Python', 'R', 'C', 'Scala', 'Rugby', 'SQL' ])\n",
        "# Mostrar los elementos del RDD lenguajes\n",
        "(\"Lenguajes de programación:\", lenguajes.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF11zbboIvci",
        "outputId": "81d966b7-4349-4d49-87f2-aa673b84223e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Lenguajes de programación:', ['Python', 'R', 'C', 'Scala', 'Rugby', 'SQL'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2: Cree un nuevo RDD que solo contenga aquellos lenguajes de programación que comiencen con la letra R."
      ],
      "metadata": {
        "id": "U24i2f3-JAh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un nuevo RDD que solo contenga aquellos lenguajes de programación que comiencen con la letra R.\n",
        "rdd_r = lenguajes.filter(lambda x: x.startswith('R'))\n",
        "rdd_r.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmdzfMLTIlRL",
        "outputId": "c66c8534-72d6-4464-c07e-d9ac412adda2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['R', 'Rugby']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Cree un RDD llamado pares que contenga los números pares existentes en el intervalo [20;30]."
      ],
      "metadata": {
        "id": "Y6-_XjhcVHW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cree un RDD llamado pares que contenga los números pares existentes en el intervalo [20;30].\n",
        "pares = sc.parallelize([20, 21, 22, 23 , 24, 25, 26, 27, 28 ,29, 30]).filter(lambda x: x % 2 == 0)\n",
        "\n",
        "pares.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlY-yNSEVNcH",
        "outputId": "7c661ae6-eb04-4ab0-8a1c-ebea437bcd3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20, 22, 24, 26, 28, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1: Obtenga una lista compuesta por los números pares en el intervalo [20;30] y sus respectivas raíces cuadradas. Un ejemplo del resultado deseado para el intervalo [50;60] sería la lista [50, 7.0710678118654755, 52, 7.211102550927978, 54, 7.3484692283495345, 56, 7.483314773547883, 58, 7.615773105863909, 60, 7.745966692414834]"
      ],
      "metadata": {
        "id": "7XwmJl5rVh1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeros_pares = [x for x in range(20, 31) if x % 2 == 0]\n",
        "pares_rdd = sc.parallelize(numeros_pares)\n",
        "pares_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694wJUl1aKUb",
        "outputId": "92aeaa19-efaa-4d7d-c76d-765cc3d6ab8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20, 22, 24, 26, 28, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2: Eleve el número de particiones del RDD sqrt a 20.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pYrypmTGa744"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Elevar el número de particiones del RDD 'pares_con_raiz' a 20\n",
        "pares_rdd = pares_rdd.repartition(20)\n",
        "\n",
        "# Verificar el número de particiones\n",
        "pares_rdd.getNumPartitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg1umbiU1GbR",
        "outputId": "d8b0a719-095e-4c8b-dfa9-99e4ebbb3825"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3: Si tuviera que disminuir el número de particiones luego de haberlo establecido en 20, ¿qué función utilizaría para hacer más eficiente su código?"
      ],
      "metadata": {
        "id": "y_hnSiSm8NYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disminuir el número de particiones a 10\n",
        "#Es la opción más adecuada cuando se desea reducir el número de particiones de un RDD ya que minimiza la sobrecarga de la red y el procesamiento, lo que conduce a una ejecución más rápida y eficiente.\n",
        "rdd_10_particiones = rdd_20_particiones.coalesce(10)"
      ],
      "metadata": {
        "id": "OMEsuGTh-qEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cree un RDD del tipo clave valor a partir de los datos adjuntos como recurso a esta lección."
      ],
      "metadata": {
        "id": "lLsEMnSJAf4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar findspark\n",
        "!pip install findspark\n",
        "\n",
        "# Importar findspark y SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Transacciones\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Crear una lista de tuplas con los datos de transacciones\n",
        "datos_transacciones = [\n",
        "    (1001, 52.3),\n",
        "    (1005, 20.8),\n",
        "    (1001, 10.1),\n",
        "    (1004, 52.7),\n",
        "    (1005, 20.7),\n",
        "    (1002, 85.3),\n",
        "    (1004, 20.9)\n",
        "]\n",
        "\n",
        "# Crear el RDD rdd_transacciones a partir de la lista de tuplas\n",
        "rdd_transacciones = spark.sparkContext.parallelize(datos_transacciones)\n",
        "\n",
        "# Ejecutar y mostrar el contenido del RDD\n",
        "rdd_transacciones.collect()\n",
        "\n",
        "# Obtener el monto total por cada cuenta (Suma)\n",
        "RDD_CUENTATOTAL = rdd_transacciones.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Mostrar el monto total por cuenta\n",
        "RDD_CUENTATOTAL.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cddBOqG_8H",
        "outputId": "001787af-0797-4b91-c094-f5533c58d057"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1004, 73.6), (1002, 85.3), (1001, 62.4), (1005, 41.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}